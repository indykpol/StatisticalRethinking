---
title: "Chapter 6 examples"
output: html_notebook
---

```{r}
library(rethinking)
```


```{r}
sppnames<-c("afarensis","africanus","habilis","boisei", "rudolfensis","ergaster","sapiens")
brainvolcc<-c(438,452,612,521,752,871,1350)
masskg<-c(37.0,35.5,34.5,41.5,55.5,61.0,53.5)
d<-data.frame(species=sppnames,brain=brainvolcc,mass=masskg)
d
```

```{r}
m6.1 <- lm( brain ~ mass , data=d )
summary(m6.1)
```
```{r}
1 - var(resid(m6.1)) / var(d$brain)
```
```{r}
m6.2 <- lm( brain ~ mass + I(mass^2) , data=d )
m6.3 <- lm( brain ~ mass + I(mass^2) + I(mass^3) , data=d )
m6.4 <- lm( brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) , data=d )
m6.5 <- lm( brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) + I(mass^5) , data=d )
m6.6 <- lm( brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) + I(mass^5) + I(mass^6) , data=d )
models <- list(m6.1, m6.2, m6.3, m6.4, m6.5, m6.6)
```

```{r}
sapply(1:length(models), FUN = function(x) summary(models[[x]])$r.squared)
```
```{r}
m6.7 <- lm(brain ~1, data = d)
summary(m6.7)
```
Information : The reduction in uncertainty derived from learning an outcome.
Information entropy H(p) as a measure of uncertainty
```{r}
p <- c(0.3, 0.7)
-sum( p * log(p) )
```
Divergence (Kullback-Leibler): The additional uncertainty induced by using probabilities from one distribution to describe another distribution.
Divergence really is measuring how far q is from the target p , in units of entropy.

```{r}
#compute deviance byc heating 
(-2) * logLik(m6.1)
```

Overthinking: Computing deviance
```{r}
# standardize the mass before fitting
d$mass.s <- (d$mass-mean(d$mass))/sd(d$mass)
m6.8 <- map( alist( 
	brain ~ dnorm( mu , sigma ) ,
	mu <- a + b*mass.s
	) , data=d , 
	start=list(a=mean(d$brain),b=0,sigma=sd(d$brain)) ,
	method="Nelder-Mead" ) 
# extract MAP estimates
theta<-coef(m6.8)

#compute deviance 
dev <- (-2)*sum( dnorm(
	d$brain , 
	mean=theta[1]+theta[2]*d$mass.s , 
	sd=theta[3] , 
	log=TRUE ) ) 
dev
```
```{r}
precis(m6.8)
```
Ov erthinking: Simulated training and testing
```{r}
N <- 20 
kseq <- 1:5 
dev <- sapply(kseq, function(k){ 
	print(k); 
	r <- replicate(1e3,sim.train.test(N=N,k=k));
	c(mean(r[1,]),mean(r[2,]),sd(r[1,]),sd(r[2,])) 
} )
plot(1:5, dev[1,], ylim=c(min(dev[1:2,])-5, max(dev[1:2,])+10), xlim=c(1,5.1), xlab="number of parameters", ylab="deviance", pch=16, col=rangi2) 
mtext(concat("N=",N))
points( (1:5)+0.1 , dev[2,] ) 

for(i in kseq) {
	pts_in<-dev[1,i]+c(-1,+1)*dev[3,i]
	pts_out<-dev[2,i]+c(-1,+1)*dev[4,i]
	lines(c(i,i),pts_in,col=rangi2)
	lines(c(i,i)+0.1,pts_out)
}
```
AIC is an approximation that is reliable only when:
(1) The priors are flat oro verwhelmed by the likelihood.
(2) The posterior distribution is approximately multivariate Gaussian.
(3) The sample size N is much greater than the number of parameters k.

DIC Deviance Information Criterion - assumes posterior is multivariate Gaussian, permits informative priors
WAIC Widely Applicable Information Criterion - makes no assumptions abour priors and shape of posterior

### Model Comparison
```{r}
data(milk)
d <- milk[complete.cases(milk),]
d$neocortex <- d$neocortex.perc/100
dim(d)
```

```{r}
a.start<-mean(d$kcal.per.g)
sigma.start<-log(sd(d$kcal.per.g)) 
m6.11 <- map( alist( 
	kcal.per.g~dnorm(a,exp(log.sigma)) ) , 
	data=d , start=list(a=a.start,log.sigma=sigma.start) ) 
m6.12 <- map( alist(
	kcal.per.g~dnorm(mu,exp(log.sigma)),
	mu<-a+bn*neocortex ) ,
	data=d , start=list(a=a.start,bn=0,log.sigma=sigma.start) )
m6.13 <- map( alist(
	kcal.per.g~dnorm(mu,exp(log.sigma)), 
	mu <- a + bm*log(mass) ) ,
	data=d , start=list(a=a.start,bm=0,log.sigma=sigma.start) ) 
m6.14 <- map( alist(
	kcal.per.g~dnorm(mu,exp(log.sigma)), 
	mu<-a+bn*neocortex+bm*log(mass)) ,
	data=d , start=list(a=a.start,bn=0,bm=0,log.sigma=sigma.start) )
```
Comparing WAIC values
```{r}
WAIC(m6.14)
```
```{r}
(milk.models<-compare(m6.11,m6.12,m6.13,m6.14))
```
A modelâ€™s weight is an estimate of the probability that the model will make the best predictions on new data, conditional on the set of models considered.
```{r}
plot( milk.models , SE=TRUE , dSE=TRUE )
```
```{r}
coeftab(m6.11,m6.12,m6.13,m6.14, se = TRUE)
```
```{r}
plot(coeftab(m6.11,m6.12,m6.13,m6.14))
```

### Model Averaging
```{r}
#compute counterfactualp redictions 
#neocortex from 0.5 to 0.8
nc.seq<-seq(from=0.5,to=0.8,length.out=30)
d.predict<-list( 
	kcal.per.g=rep(0,30),# empty outcome 
	neocortex=nc.seq, # sequence of neocortex 
	mass=rep(4.5,30) ) # average mass 
pred.m6.14<-link(m6.14,data=d.predict)
mu <- apply( pred.m6.14 , 2 , mean ) 
mu.PI <- apply( pred.m6.14 , 2 , PI ) 

# plot it all 
plot(kcal.per.g~neocortex,d,col=rangi2) 
lines(nc.seq,mu,lty=2) 
lines(nc.seq,mu.PI[1,],lty=2) 
lines(nc.seq,mu.PI[2,],lty=2)

# Ensemble model now:
milk.ensemble<-ensemble(m6.11,m6.12,m6.13,m6.14,data=d.predict)
mu <- apply( milk.ensemble$link , 2 , mean ) 
mu.PI <- apply( milk.ensemble$link , 2 , PI ) 
lines(nc.seq,mu)
shade(mu.PI,nc.seq)
lines(nc.seq,mu)
shade(mu.PI,nc.seq)
```